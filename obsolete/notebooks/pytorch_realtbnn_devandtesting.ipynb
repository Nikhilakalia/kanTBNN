{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9df27f-484c-45c8-a423-6c31cfd7cd60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e480687c-75eb-4654-a827-1e6eaba0adbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = torch.tensor([[1,2,3],[2,-0.5,5],[3,5,-0.5]])\n",
    "g1 = torch.tensor([0.1])\n",
    "Tn = torch.tensor([[[1,2,3],[2,-0.5,5],[3,5,-0.5]],[[-1,2,3],[2,0.5,5],[3,5,0.5]]])\n",
    "gn = torch.tensor([0.1,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ba86d0-2fe1-42a6-935f-8cf0c085bddd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000,  2.0000,  3.0000],\n",
      "        [ 2.0000, -0.5000,  5.0000],\n",
      "        [ 3.0000,  5.0000, -0.5000]])\n",
      "tensor([0.1000])\n",
      "tensor([[ 0.1000,  0.2000,  0.3000],\n",
      "        [ 0.2000, -0.0500,  0.5000],\n",
      "        [ 0.3000,  0.5000, -0.0500]])\n",
      "tensor([[[ 1.0000,  2.0000,  3.0000],\n",
      "         [ 2.0000, -0.5000,  5.0000],\n",
      "         [ 3.0000,  5.0000, -0.5000]],\n",
      "\n",
      "        [[-1.0000,  2.0000,  3.0000],\n",
      "         [ 2.0000,  0.5000,  5.0000],\n",
      "         [ 3.0000,  5.0000,  0.5000]]])\n",
      "torch.Size([2, 3, 3])\n",
      "tensor([0.1000, 0.2000])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 1, 1])\n",
      "tensor([[[0.1000]],\n",
      "\n",
      "        [[0.2000]]])\n",
      "tensor([[[ 0.1000,  0.2000,  0.3000],\n",
      "         [ 0.2000, -0.0500,  0.5000],\n",
      "         [ 0.3000,  0.5000, -0.0500]],\n",
      "\n",
      "        [[-0.2000,  0.4000,  0.6000],\n",
      "         [ 0.4000,  0.1000,  1.0000],\n",
      "         [ 0.6000,  1.0000,  0.1000]]])\n",
      "tensor([[[0.1000, 0.1000, 0.1000],\n",
      "         [0.1000, 0.1000, 0.1000],\n",
      "         [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "        [[0.2000, 0.2000, 0.2000],\n",
      "         [0.2000, 0.2000, 0.2000],\n",
      "         [0.2000, 0.2000, 0.2000]]])\n",
      "tensor([[[ 0.1000,  0.2000,  0.3000],\n",
      "         [ 0.2000, -0.0500,  0.5000],\n",
      "         [ 0.3000,  0.5000, -0.0500]],\n",
      "\n",
      "        [[-0.2000,  0.4000,  0.6000],\n",
      "         [ 0.4000,  0.1000,  1.0000],\n",
      "         [ 0.6000,  1.0000,  0.1000]]])\n",
      "tensor([[-0.1000,  0.6000,  0.9000],\n",
      "        [ 0.6000,  0.0500,  1.5000],\n",
      "        [ 0.9000,  1.5000,  0.0500]])\n"
     ]
    }
   ],
   "source": [
    "print(T1)\n",
    "print(g1)\n",
    "print(g1*T1)\n",
    "print(Tn)\n",
    "print(Tn.shape)\n",
    "print(gn)\n",
    "print(gn.shape)\n",
    "print(gn.view(-1,1,1).shape)\n",
    "print(gn.view(-1,1,1))\n",
    "print(gn.view(-1,1,1)*Tn)\n",
    "\n",
    "print(gn.view(-1,1,1)*torch.ones_like(Tn))\n",
    "print(gn.view(-1,1,1)*torch.ones_like(Tn)*Tn)\n",
    "print(torch.sum(gn.view(-1,1,1)*torch.ones_like(Tn)*Tn,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e3fbe7-ac4f-4289-8ae7-89caec88653d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.0000,  2.0000,  3.0000],\n",
      "          [ 2.0000, -0.5000,  5.0000],\n",
      "          [ 3.0000,  5.0000, -0.5000]],\n",
      "\n",
      "         [[-1.0000,  2.0000,  3.0000],\n",
      "          [ 2.0000,  0.5000,  5.0000],\n",
      "          [ 3.0000,  5.0000,  0.5000]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0000,  2.0000,  3.0000],\n",
      "          [ 2.0000, -1.0000,  5.0000],\n",
      "          [ 3.0000,  5.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000,  2.0000,  3.0000],\n",
      "          [ 2.0000,  3.0000,  7.0000],\n",
      "          [ 3.0000,  7.0000, -2.0000]]]])\n",
      "torch.Size([2, 2, 3, 3])\n",
      "tensor([[0.1000, 0.2000],\n",
      "        [0.3000, 0.5000]])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2, 1, 1])\n",
      "tensor([[[[0.1000]],\n",
      "\n",
      "         [[0.2000]]],\n",
      "\n",
      "\n",
      "        [[[0.3000]],\n",
      "\n",
      "         [[0.5000]]]])\n",
      "tensor([[[[ 0.1000,  0.2000,  0.3000],\n",
      "          [ 0.2000, -0.0500,  0.5000],\n",
      "          [ 0.3000,  0.5000, -0.0500]],\n",
      "\n",
      "         [[-0.2000,  0.4000,  0.6000],\n",
      "          [ 0.4000,  0.1000,  1.0000],\n",
      "          [ 0.6000,  1.0000,  0.1000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6000,  0.6000,  0.9000],\n",
      "          [ 0.6000, -0.3000,  1.5000],\n",
      "          [ 0.9000,  1.5000, -0.3000]],\n",
      "\n",
      "         [[-0.5000,  1.0000,  1.5000],\n",
      "          [ 1.0000,  1.5000,  3.5000],\n",
      "          [ 1.5000,  3.5000, -1.0000]]]])\n",
      "tensor([[[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.2000, 0.2000, 0.2000],\n",
      "          [0.2000, 0.2000, 0.2000],\n",
      "          [0.2000, 0.2000, 0.2000]]],\n",
      "\n",
      "\n",
      "        [[[0.3000, 0.3000, 0.3000],\n",
      "          [0.3000, 0.3000, 0.3000],\n",
      "          [0.3000, 0.3000, 0.3000]],\n",
      "\n",
      "         [[0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000]]]])\n",
      "tensor([[[[ 0.1000,  0.2000,  0.3000],\n",
      "          [ 0.2000, -0.0500,  0.5000],\n",
      "          [ 0.3000,  0.5000, -0.0500]],\n",
      "\n",
      "         [[-0.2000,  0.4000,  0.6000],\n",
      "          [ 0.4000,  0.1000,  1.0000],\n",
      "          [ 0.6000,  1.0000,  0.1000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6000,  0.6000,  0.9000],\n",
      "          [ 0.6000, -0.3000,  1.5000],\n",
      "          [ 0.9000,  1.5000, -0.3000]],\n",
      "\n",
      "         [[-0.5000,  1.0000,  1.5000],\n",
      "          [ 1.0000,  1.5000,  3.5000],\n",
      "          [ 1.5000,  3.5000, -1.0000]]]])\n",
      "gn: tensor([[0.1000, 0.2000],\n",
      "        [0.3000, 0.5000]])\n",
      "Tn: tensor([[[[ 1.0000,  2.0000,  3.0000],\n",
      "          [ 2.0000, -0.5000,  5.0000],\n",
      "          [ 3.0000,  5.0000, -0.5000]],\n",
      "\n",
      "         [[-1.0000,  2.0000,  3.0000],\n",
      "          [ 2.0000,  0.5000,  5.0000],\n",
      "          [ 3.0000,  5.0000,  0.5000]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0000,  2.0000,  3.0000],\n",
      "          [ 2.0000, -1.0000,  5.0000],\n",
      "          [ 3.0000,  5.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000,  2.0000,  3.0000],\n",
      "          [ 2.0000,  3.0000,  7.0000],\n",
      "          [ 3.0000,  7.0000, -2.0000]]]])\n",
      "output: tensor([[[-0.1000,  0.6000,  0.9000],\n",
      "         [ 0.6000,  0.0500,  1.5000],\n",
      "         [ 0.9000,  1.5000,  0.0500]],\n",
      "\n",
      "        [[ 0.1000,  1.6000,  2.4000],\n",
      "         [ 1.6000,  1.2000,  5.0000],\n",
      "         [ 2.4000,  5.0000, -1.3000]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Tn = torch.tensor([[[[1,2,3],[2,-0.5,5],[3,5,-0.5]],[[-1,2,3],[2,0.5,5],[3,5,0.5]]],\n",
    "                   [[[2,2,3],[2,-1,5],[3,5,-1]],[[-1,2,3],[2,3,7],[3,7,-2]]]])\n",
    "gn = torch.tensor([[0.1,0.2],[0.3, 0.5]])\n",
    "\n",
    "\n",
    "print(Tn)\n",
    "print(Tn.shape)\n",
    "print(gn)\n",
    "print(gn.shape)\n",
    "print(gn.view(-1,2,1,1).shape)\n",
    "print(gn.view(-1,2,1,1))\n",
    "print(gn.view(-1,2,1,1)*Tn)\n",
    "\n",
    "print(gn.view(-1,2,1,1)*torch.ones_like(Tn))\n",
    "print(gn.view(-1,2,1,1)*torch.ones_like(Tn)*Tn)\n",
    "\n",
    "print(f'gn: {gn}')\n",
    "print(f'Tn: {Tn}')\n",
    "print(f'output: {torch.sum(gn.view(-1,2,1,1)*torch.ones_like(Tn)*Tn,axis=1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3c7e8d-2aa8-42fe-aa9d-a1f3a29b22ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TBNN(nn.Module):\n",
    "    def __init__(self, N: int, input_dim: int, n_hidden: int, neurons: int):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.input_dim = input_dim   \n",
    "        \n",
    "        self.gn = nn.Linear(neurons,self.N)\n",
    "        \n",
    "        self.hidden = nn.ModuleList()\n",
    "        for k in range(n_hidden):\n",
    "            self.hidden.append(nn.Linear(input_dim, neurons))\n",
    "            input_dim = neurons  # For the next layer\n",
    "                    \n",
    "    def forward(self, x, Tn):\n",
    "        for layer in self.hidden:\n",
    "            x = F.relu(layer(x))\n",
    "        gn = self.gn(x)\n",
    "        b_pred = torch.sum(gn.view(-1,self.N,1,1)*torch.ones_like(Tn)*Tn,axis=1)\n",
    "        return b_pred, gn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f1f132-aa36-428c-a365-16623726df97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([[[-0.0986, -0.7613, -1.1419],\n",
      "         [-0.7613,  0.0493, -1.9032],\n",
      "         [-1.1419, -1.9032,  0.0493]],\n",
      "\n",
      "        [[-0.4102, -0.7586, -1.1379],\n",
      "         [-0.7586, -0.0853, -2.1288],\n",
      "         [-1.1379, -2.1288,  0.4955]]], grad_fn=<SumBackward1>)\n",
      "\n",
      "Test conditions:\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "TBNN(\n",
      "  (gn): Linear(in_features=20, out_features=2, bias=True)\n",
      "  (hidden): ModuleList(\n",
      "    (0): Linear(in_features=5, out_features=20, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=20, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tbnn = TBNN(N=2, input_dim=5, n_hidden = 3, neurons = 20)\n",
    "\n",
    "x = torch.tensor([[0.1, 0.5, 0.6, 0.3, 0.7],[-0.1, -0.5, 0.6, 0.3, -0.7]])\n",
    "Tn = torch.tensor([[[[1,2,3],[2,-0.5,5],[3,5,-0.5]],[[-1,2,3],[2,0.5,5],[3,5,0.5]]],\n",
    "                   [[[2,2,3],[2,-1,5],[3,5,-1]],[[-1,2,3],[2,3,7],[3,7,-2]]]])\n",
    "\n",
    "prediction, gn= tbnn(x,Tn)\n",
    "\n",
    "print(f'Predictions: {prediction}')\n",
    "print('\\nTest conditions:')\n",
    "for point in range(x.shape[0]):\n",
    "    for i in range(0,3):\n",
    "        for j in range(0,3):\n",
    "            print(prediction[point,i,j] == gn[point,0]*Tn[point,0,i,j] + gn[point,1]*Tn[point,1,i,j])\n",
    "print(tbnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d4466f-fad8-4f12-8acc-7ad4cc3a3752",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n",
      "First gn: tensor([[-0.1926,  0.2924],\n",
      "        [-0.1657,  0.2821]], grad_fn=<AddmmBackward0>)\n",
      "Last gn: tensor([[2.4028e-07, 3.8743e-07],\n",
      "        [2.3283e-07, 4.3958e-07]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Zero out testing: training should produce zero gn's:\n",
    "model = TBNN(N=2, input_dim=5, n_hidden = 3, neurons = 20)\n",
    "\n",
    "x = torch.tensor([[0.1, 0.5, 0.6, 0.3, 0.7],[-0.1, -0.5, 0.6, 0.3, -0.7]])\n",
    "Tn = torch.tensor([[[[1,2,3],[2,-0.5,5],[3,5,-0.5]],[[-1,2,3],[2,0.5,5],[3,5,0.5]]],\n",
    "                   [[[2,2,3],[2,-1,5],[3,5,-1]],[[-1,2,3],[2,3,7],[3,7,-2]]]])\n",
    "b_label = torch.tensor([[[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0]]])\n",
    "print(b_label.shape)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    y_pred, gn = model(x, Tn)\n",
    "    if epoch == 0:\n",
    "        print(f'First gn: {gn}')\n",
    "    loss = loss_fn(y_pred.float(), b_label.float())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'Last gn: {gn}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a2a2a6f-b89a-478b-b8b3-90f0b4db2b16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0(a) condition: True, first gn: [0.2579 0.2652],[-0.0976 -0.1416] last gn: [1.0001 1.0001],[-2.1905e-05  4.9412e-05]\n",
      "Test 0(b) condition: True, first gn: [-0.2216 -0.1881],[0.0009 0.0145] last gn: [-1.4901e-08 -1.4901e-08],[1.0000 1.0000]\n",
      "Test 1(a) condition: True, first gn: [0.2627 0.2468],[-0.2274 -0.2500] last gn: [1.0000 1.0000],[-1.4901e-08 -1.4901e-08]\n",
      "Test 1(b) condition: True, first gn: [0.2237 0.2256],[-0.1831 -0.1758] last gn: [0.0000 0.0000],[1.0000 1.0000]\n",
      "Test 2(a) condition: True, first gn: [0.1724 0.1887],[0.1925 0.2081] last gn: [1.0000 1.0000],[0.0000 0.0000]\n",
      "Test 2(b) condition: True, first gn: [0.1433 0.1247],[-0.1860 -0.2005] last gn: [-3.7253e-09  1.4901e-08],[1.0000 1.0000]\n",
      "Test 3(a) condition: True, first gn: [-0.0790 -0.0726],[-0.0546 -0.0875] last gn: [1.0000 1.0000],[0.0000 0.0000]\n",
      "Test 3(b) condition: True, first gn: [0.1983 0.1976],[0.0106 0.0191] last gn: [-2.9802e-08 -2.9802e-08],[1.0000 1.0000]\n",
      "Test 4(a) condition: True, first gn: [0.1532 0.1659],[-0.1513 -0.1456] last gn: [1.0000 1.0000],[-1.4901e-08 -1.4901e-08]\n",
      "Test 4(b) condition: True, first gn: [-0.1480 -0.1469],[-0.0184 -0.0207] last gn: [-2.9802e-08 -2.9802e-08],[1.0000 1.0000]\n",
      "Test 5(a) condition: True, first gn: [-0.0540 -0.0517],[0.1316 0.1300] last gn: [1.0000 1.0000],[0.0000 0.0000]\n",
      "Test 5(b) condition: True, first gn: [-0.0693 -0.0940],[-0.0794 -0.0990] last gn: [ 4.0531e-06 -1.5497e-06],[1.0000 1.0000]\n",
      "Test 6(a) condition: True, first gn: [-0.1366 -0.1366],[-0.0888 -0.0823] last gn: [1.0000 1.0000],[0.0000 0.0000]\n",
      "Test 6(b) condition: True, first gn: [-0.1041 -0.1420],[0.1097 0.0994] last gn: [-2.9802e-08 -2.9802e-08],[1.0000 1.0000]\n",
      "Test 7(a) condition: True, first gn: [0.1839 0.1571],[-0.0853 -0.0824] last gn: [1.0000 1.0000],[-2.9802e-08  0.0000e+00]\n",
      "Test 7(b) condition: True, first gn: [-0.2440 -0.2623],[-0.2527 -0.1898] last gn: [2.9802e-08 2.9802e-08],[1.0000 1.0000]\n",
      "Test 8(a) condition: True, first gn: [0.1241 0.1346],[-0.0217 -0.0754] last gn: [1.0000 1.0000],[0.0000 0.0000]\n",
      "Test 8(b) condition: True, first gn: [0.1032 0.0968],[-0.2110 -0.1961] last gn: [-2.9802e-08 -2.9802e-08],[1.0000 1.0000]\n",
      "Test 9(a) condition: True, first gn: [-0.1484 -0.1427],[0.1425 0.1406] last gn: [1.0000 1.0000],[1.4901e-08 1.4901e-08]\n",
      "Test 9(b) condition: True, first gn: [0.1260 0.1384],[0.2142 0.2027] last gn: [0.0000 0.0000],[1.0000 1.0000]\n"
     ]
    }
   ],
   "source": [
    "# One testing: training should produce gn = 1, 0 in the first case, and 0, 1 in the second case\n",
    "np.set_printoptions(precision=4,floatmode='fixed')\n",
    "\n",
    "for test in range(10):\n",
    "    model = TBNN(N=2, input_dim=5, n_hidden = 3, neurons = 20)\n",
    "\n",
    "    x = torch.tensor([[0.1, 0.5, 0.6, 0.3, 0.7],[-0.1, -0.5, 0.6, 0.3, -0.7]])\n",
    "    Tn = torch.tensor([[[[1,2,3],[2,-0.5,5],[3,5,-0.5]],[[-1,2,3],[2,0.5,5],[3,5,0.5]]],\n",
    "                       [[[2,2,3],[2,-1,5],[3,5,-1]],[[-1,2,3],[2,3,7],[3,7,-2]]]])\n",
    "    b_label = torch.tensor([[[1,2,3],[2,-0.5,5],[3,5,-0.5]],[[2,2,3],[2,-1,5],[3,5,-1]]])\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "    for epoch in range(2000):\n",
    "        model.train()\n",
    "        y_pred, gn = model(x, Tn)\n",
    "        if epoch == 0:\n",
    "            gn0 = gn\n",
    "        loss = loss_fn(y_pred.float(), b_label.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Test {test}(a) condition: {(np.abs(np.mean(np.abs(gn[:,0].detach().numpy()- 1))) < 1E-3) & (np.abs(np.mean(np.abs(gn[:,1].detach().numpy()))) < 1E-3)}, first gn: {gn0[:,0].detach().numpy()},{gn0[:,1].detach().numpy()} last gn: {gn[:,0].detach().numpy()},{gn[:,1].detach().numpy()}')\n",
    "\n",
    "    model = TBNN(N=2, input_dim=5, n_hidden = 3, neurons = 20)\n",
    "\n",
    "    x = torch.tensor([[0.1, 0.5, 0.6, 0.3, 0.7],[-0.1, -0.5, 0.6, 0.3, -0.7]])\n",
    "    Tn = torch.tensor([[[[1,2,3],[2,-0.5,5],[3,5,-0.5]],[[-1,2,3],[2,0.5,5],[3,5,0.5]]],\n",
    "                       [[[2,2,3],[2,-1,5],[3,5,-1]],[[-1,2,3],[2,3,7],[3,7,-2]]]])\n",
    "    b_label = torch.tensor([[[-1,2,3],[2,0.5,5],[3,5,0.5]],[[-1,2,3],[2,3,7],[3,7,-2]]])\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "    for epoch in range(2000):\n",
    "        model.train()\n",
    "        y_pred, gn = model(x, Tn)\n",
    "        if epoch == 0:\n",
    "            gn0 = gn\n",
    "        loss = loss_fn(y_pred.float(), b_label.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    print(f'Test {test}(b) condition: {(np.abs(np.mean(np.abs(gn[:,1].detach().numpy()-1))) < 1E-3) & (np.abs(np.mean(np.abs(gn[:,0].detach().numpy()))) < 1E-3)}, first gn: {gn0[:,0].detach().numpy()},{gn0[:,1].detach().numpy()} last gn: {gn[:,0].detach().numpy()},{gn[:,1].detach().numpy()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d15c0be2-dadf-4f27-b3be-9b57fde9ad6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.0002609304792713374\n",
      "\n",
      "\n",
      "Truth, prediction: -0.686869001091581,-0.6925135188679767, error 0.005644517776395741\n",
      "Truth, prediction: 0.21012973521889,0.1776430688480815, error 0.03248666637080849\n",
      "Truth, prediction: 0.6922745508430619,0.7044069998623845, error 0.012132449019322644\n",
      "Truth, prediction: 0.21012973521889,0.1776430688480815, error 0.03248666637080849\n",
      "Truth, prediction: 0.8564613655566735,0.8683485093056025, error 0.011887143748928963\n",
      "Truth, prediction: -0.10033681303563813,-0.10756201803122628, error 0.0072252049955881525\n",
      "Truth, prediction: 0.6922745508430619,0.7044069998623845, error 0.012132449019322644\n",
      "Truth, prediction: -0.10033681303563813,-0.10756201803122628, error 0.0072252049955881525\n",
      "Truth, prediction: -0.16959236446509252,-0.17583499043762596, error 0.006242625972533444\n",
      "\n",
      "\n",
      "Truth, prediction: 0.887716499093528,0.8762551631355262, error 0.011461335958001806\n",
      "Truth, prediction: 0.5485454603598465,0.5440657231584218, error 0.004479737201424694\n",
      "Truth, prediction: -0.8549895604794804,-0.8655168871829345, error 0.010527326703454132\n",
      "Truth, prediction: 0.5485454603598465,0.5440657231584218, error 0.004479737201424694\n",
      "Truth, prediction: 0.4632005544312059,0.47756577107014964, error 0.014365216638943745\n",
      "Truth, prediction: -0.056140751279952994,-0.056110384666961804, error 3.036661299118948e-05\n",
      "Truth, prediction: -0.8549895604794804,-0.8655168871829345, error 0.010527326703454132\n",
      "Truth, prediction: -0.056140751279952994,-0.056110384666961804, error 3.036661299118948e-05\n",
      "Truth, prediction: -1.350917053524734,-1.353820934205676, error 0.0029038806809420503\n",
      "\n",
      "\n",
      "Truth, prediction: -0.8400825317497231,-0.8443499773108198, error 0.0042674455610967055\n",
      "Truth, prediction: 0.5512302655955543,0.5671604436359277, error 0.015930178040373333\n",
      "Truth, prediction: 0.9013314993572634,0.933802358815762, error 0.03247085945849859\n",
      "Truth, prediction: 0.5512302655955543,0.5671604436359277, error 0.015930178040373333\n",
      "Truth, prediction: -0.6847774607555539,-0.6769068466554002, error 0.007870614100153661\n",
      "Truth, prediction: -0.3094351294689832,-0.3218658026436377, error 0.01243067317465446\n",
      "Truth, prediction: 0.9013314993572634,0.933802358815762, error 0.03247085945849859\n",
      "Truth, prediction: -0.3094351294689832,-0.3218658026436377, error 0.01243067317465446\n",
      "Truth, prediction: 1.524859992505277,1.52125682396622, error 0.0036031685390569557\n",
      "\n",
      "\n",
      "Truth, prediction: -0.8568992382806369,-0.8574291401818416, error 0.0005299019012047212\n",
      "Truth, prediction: -0.5814982755041371,-0.6058272456526609, error 0.024328970148523754\n",
      "Truth, prediction: 0.23662938911786635,0.22907966682797593, error 0.007549722289890426\n",
      "Truth, prediction: -0.5814982755041371,-0.6058272456526609, error 0.024328970148523754\n",
      "Truth, prediction: 0.5923410211057609,0.584319258506613, error 0.008021762599147886\n",
      "Truth, prediction: -0.24884764289756367,-0.24428121274226489, error 0.004566430155298784\n",
      "Truth, prediction: 0.23662938911786635,0.22907966682797593, error 0.007549722289890426\n",
      "Truth, prediction: -0.24884764289756367,-0.24428121274226489, error 0.004566430155298784\n",
      "Truth, prediction: 0.26455821717487593,0.27310988167522854, error 0.008551664500352607\n",
      "\n",
      "\n",
      "Truth, prediction: -0.7932352295202791,-0.8269343284995803, error 0.03369909897930112\n",
      "Truth, prediction: -0.7432641602093608,-0.7123999883523279, error 0.03086417185703294\n",
      "Truth, prediction: 0.5833890341123118,0.5586865573852243, error 0.024702476727087475\n",
      "Truth, prediction: -0.7432641602093608,-0.7123999883523279, error 0.03086417185703294\n",
      "Truth, prediction: 0.35117133292015734,0.3535991387017896, error 0.002427805781632264\n",
      "Truth, prediction: 0.6954811388769029,0.701586936457869, error 0.006105797580966077\n",
      "Truth, prediction: 0.5833890341123118,0.5586865573852243, error 0.024702476727087475\n",
      "Truth, prediction: 0.6954811388769029,0.701586936457869, error 0.006105797580966077\n",
      "Truth, prediction: 0.4420638966001218,0.4733351897977906, error 0.031271293197668804\n",
      "\n",
      "\n",
      "Truth, prediction: 0.059949713902606616,0.06463922506094288, error 0.004689511158336268\n",
      "Truth, prediction: 0.3515130109202498,0.35054580916164535, error 0.0009672017586044301\n",
      "Truth, prediction: -0.896017662233892,-0.8990333134630638, error 0.0030156512291718007\n",
      "Truth, prediction: 0.3515130109202498,0.35054580916164535, error 0.0009672017586044301\n",
      "Truth, prediction: -0.6651163469408203,-0.6768925159422408, error 0.011776169001420422\n",
      "Truth, prediction: 0.3412115304351584,0.34500201394223623, error 0.0037904835070778353\n",
      "Truth, prediction: -0.896017662233892,-0.8990333134630638, error 0.0030156512291718007\n",
      "Truth, prediction: 0.3412115304351584,0.34500201394223623, error 0.0037904835070778353\n",
      "Truth, prediction: 0.6051666330382137,0.6122532908812978, error 0.007086657843084043\n",
      "\n",
      "\n",
      "Truth, prediction: 0.9148945740007595,0.9092440869330792, error 0.005650487067680232\n",
      "Truth, prediction: -0.8026562026977686,-0.7832129931228602, error 0.019443209574908416\n",
      "Truth, prediction: -0.5866745567600609,-0.5734124193696108, error 0.013262137390450013\n",
      "Truth, prediction: -0.8026562026977686,-0.7832129931228602, error 0.019443209574908416\n",
      "Truth, prediction: -0.40735710638176204,-0.40670980222439856, error 0.0006473041573634708\n",
      "Truth, prediction: 0.837978518951594,0.8314376891912703, error 0.006540829760323774\n",
      "Truth, prediction: -0.5866745567600609,-0.5734124193696108, error 0.013262137390450013\n",
      "Truth, prediction: 0.837978518951594,0.8314376891912703, error 0.006540829760323774\n",
      "Truth, prediction: -0.5075374676189974,-0.5025342847086809, error 0.005003182910316539\n",
      "\n",
      "\n",
      "Truth, prediction: 0.15836887846523018,0.15946691323421408, error 0.001098034768983902\n",
      "Truth, prediction: -0.41066983250520495,-0.42062249244316013, error 0.009952659937955188\n",
      "Truth, prediction: 0.3423965445763646,0.34720295413104674, error 0.004806409554682145\n",
      "Truth, prediction: -0.41066983250520495,-0.42062249244316013, error 0.009952659937955188\n",
      "Truth, prediction: -0.6481603025482383,-0.643340678065784, error 0.004819624482454321\n",
      "Truth, prediction: 0.3099494583262523,0.306706646142179, error 0.003242812184073296\n",
      "Truth, prediction: 0.3423965445763646,0.34720295413104674, error 0.004806409554682145\n",
      "Truth, prediction: 0.3099494583262523,0.306706646142179, error 0.003242812184073296\n",
      "Truth, prediction: 0.48979142408300813,0.48387376483156963, error 0.005917659251438501\n",
      "\n",
      "\n",
      "Truth, prediction: 0.15014381532604792,0.10923169055436191, error 0.040912124771686015\n",
      "Truth, prediction: 0.7116876126792815,0.7021463145599623, error 0.009541298119319142\n",
      "Truth, prediction: -0.3345826837063821,-0.33415179660161304, error 0.0004308871047690732\n",
      "Truth, prediction: 0.7116876126792815,0.7021463145599623, error 0.009541298119319142\n",
      "Truth, prediction: -0.22802247887544502,-0.25689809499940425, error 0.028875616123959225\n",
      "Truth, prediction: -0.1430229523995128,-0.14542464733185134, error 0.0024016949323385495\n",
      "Truth, prediction: -0.3345826837063821,-0.33415179660161304, error 0.0004308871047690732\n",
      "Truth, prediction: -0.1430229523995128,-0.14542464733185134, error 0.0024016949323385495\n",
      "Truth, prediction: 0.0778786635493971,0.14766640444504242, error 0.06978774089564532\n",
      "\n",
      "\n",
      "Truth, prediction: -0.4369234348656861,-0.4300799225482733, error 0.006843512317412759\n",
      "Truth, prediction: 0.6924825404651509,0.7046344769604279, error 0.012151936495277016\n",
      "Truth, prediction: -0.6296410292331713,-0.622076385000567, error 0.0075646442326043895\n",
      "Truth, prediction: 0.6924825404651509,0.7046344769604279, error 0.012151936495277016\n",
      "Truth, prediction: 0.13457963854094324,0.13925419725567123, error 0.004674558714727994\n",
      "Truth, prediction: 0.3389526378631045,0.32629423449855155, error 0.012658403364552928\n",
      "Truth, prediction: -0.6296410292331713,-0.622076385000567, error 0.0075646442326043895\n",
      "Truth, prediction: 0.3389526378631045,0.32629423449855155, error 0.012658403364552928\n",
      "Truth, prediction: 0.30234379632474284,0.2908257252926021, error 0.011518071032140753\n"
     ]
    }
   ],
   "source": [
    "# Memorization testing: model should be able to memorize a random data point\n",
    "np.set_printoptions(precision=4,floatmode='fixed')\n",
    "test = 0\n",
    "tensor_N = 10\n",
    "br = np.random.uniform(-1,1,size=(2,5))\n",
    "N_points = 10\n",
    "\n",
    "def generate_random_antisymmetric_tensors(n_points):\n",
    "    b = np.empty((n_points,3,3))\n",
    "    br = np.random.uniform(-1,1,size=(n_points,5))\n",
    "    for point in range(n_points):\n",
    "        b[point,0,0] = br[point,0]\n",
    "        b[point,0,1] = br[point,1]\n",
    "        b[point,0,2] = br[point,2]\n",
    "        b[point,1,1] = br[point,3]\n",
    "        b[point,1,2] = br[point,4]\n",
    "        b[point,2,2] = -b[point,0,0] - b[point,1,1]\n",
    "        b[point,1,0] = b[point,0,1]\n",
    "        b[point,2,0] = b[point,0,2]\n",
    "        b[point,2,1] = b[point,1,2]\n",
    "    return b\n",
    "\n",
    "model = TBNN(N=tensor_N, input_dim=5, n_hidden = 5, neurons = 100)\n",
    "\n",
    "Tn = np.empty((N_points,tensor_N,3,3))\n",
    "for point in range(N_points):\n",
    "    for n in range(tensor_N):\n",
    "        Tn[point,n,:,:] = generate_random_antisymmetric_tensors(1)\n",
    "        #print(Tn[0,n,:,:] )\n",
    "x = torch.tensor(np.random.uniform(-1,1,size=(N_points,5)),dtype = torch.float32)\n",
    "Tn = torch.tensor(Tn)\n",
    "b_label = torch.tensor(generate_random_antisymmetric_tensors(N_points))\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(1500):\n",
    "    model.train()\n",
    "    y_pred, gn = model(x, Tn)\n",
    "    if epoch == 0:\n",
    "        gn0 = gn\n",
    "    loss = loss_fn(y_pred.float(), b_label.float())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "predictions,gn = model(x,Tn)\n",
    "loss = loss_fn(predictions.float(), b_label.float())\n",
    "print(f'LOSS: {loss}')\n",
    "for point in range(N_points): \n",
    "    print('\\n')\n",
    "    for i in range(0,3):\n",
    "        for j in range(0,3):\n",
    "            print(f'Truth, prediction: {b_label[point][i,j]},{predictions[point][i,j]}, error {abs(b_label[point][i,j] - predictions[point][i,j])}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "86014cc5-57db-428d-afcc-4b4fcad02c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def myCustomLoss(my_outputs, my_labels):\n",
    "    #specifying the batch size\n",
    "    my_batch_size = my_outputs.size()[0]\n",
    "    #calculating the log of softmax values           \n",
    "    #my_outputs = ((my_outputs-my_labels)**2).mean()\n",
    "    #selecting the values that correspond to labels\n",
    "    #my_outputs = my_outputs[range(my_batch_size), my_labels]\n",
    "    #returning the results\n",
    "    return ((my_outputs-my_labels)**2).mean()\n",
    "\n",
    "def bLoss_equiv_mse(outputs, labels):\n",
    "    #specifying the batch size\n",
    "    batch_size = outputs.size()[0]\n",
    "    se = ((outputs[:,0,0] - labels[:,0,0])**2 \\\n",
    "           + 2*(outputs[:,0,1] - labels[:,0,1])**2 \\\n",
    "           + 2*(outputs[:,0,2] - labels[:,0,2])**2 \\\n",
    "           + (outputs[:,1,1] - labels[:,1,1])**2 \\\n",
    "           + 2*(outputs[:,1,2] - labels[:,1,2])**2 \\\n",
    "           + (outputs[:,2,2] - labels[:,2,2])**2 \\\n",
    "          )/9\n",
    "    \n",
    "    #mse = ((torch.diagonal(outputs, dim1=1, dim2=2) - torch.diagonal(labels, dim1=1, dim2=2))**2 + (torch.diagonal(outputs,offset=1, dim1=1, dim2=2) - torch.diagonal(labels, offset=1,dim1=1, dim2=2))**2).mean()\n",
    "    #calculating the log of softmax values           \n",
    "    #my_outputs = ((my_outputs-my_labels)**2).mean()\n",
    "    #selecting the values that correspond to labels\n",
    "    #my_outputs = my_outputs[range(my_batch_size), my_labels]\n",
    "    #returning the results\n",
    "    return se.mean()\n",
    "\n",
    "def bLoss(outputs, labels, alpha = 1):\n",
    "    #specifying the batch size\n",
    "    batch_size = outputs.size()[0]\n",
    "    se = ((outputs[:,0,0] - labels[:,0,0])**2 \\\n",
    "           + (outputs[:,0,1] - labels[:,0,1])**2 \\\n",
    "           + (outputs[:,0,2] - labels[:,0,2])**2 \\\n",
    "           + (outputs[:,1,1] - labels[:,1,1])**2 \\\n",
    "           + (outputs[:,1,2] - labels[:,1,2])**2 \\\n",
    "           + (outputs[:,2,2] - labels[:,2,2])**2 \\\n",
    "          )/6\n",
    "    zero = torch.zeros_like(outputs)\n",
    "    re =  (torch.maximum(torch.maximum(outputs[:,0,0]-2/3, -(outputs[:,0,0] + 1/3)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,1,1]-2/3, -(outputs[:,1,1] + 1/3)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,2,2]-2/3, -(outputs[:,2,2] + 1/3)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,0,1]-1/2, -(outputs[:,0,1] + 1/2)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,0,2]-1/2, -(outputs[:,0,2] + 1/2)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,1,2]-1/2, -(outputs[:,1,2] + 1/2)), zero[:,0,0])**2 \\\n",
    "          )/6\n",
    "\n",
    "    #mse = ((torch.diagonal(outputs, dim1=1, dim2=2) - torch.diagonal(labels, dim1=1, dim2=2))**2 + (torch.diagonal(outputs,offset=1, dim1=1, dim2=2) - torch.diagonal(labels, offset=1,dim1=1, dim2=2))**2).mean()\n",
    "    #calculating the log of softmax values           \n",
    "    #my_outputs = ((my_outputs-my_labels)**2).mean()\n",
    "    #selecting the values that correspond to labels\n",
    "    #my_outputs = my_outputs[range(my_batch_size), my_labels]\n",
    "    #returning the results\n",
    "    return (se+alpha*re).mean()\n",
    "\n",
    "def realizabilityLoss(outputs, labels, alpha = 1):\n",
    "    zero = torch.zeros_like(outputs)\n",
    "    re =  (torch.maximum(torch.maximum(outputs[:,0,0]-2/3, -(outputs[:,0,0] + 1/3)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,1,1]-2/3, -(outputs[:,1,1] + 1/3)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,2,2]-2/3, -(outputs[:,2,2] + 1/3)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,0,1]-1/2, -(outputs[:,0,1] + 1/2)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,0,2]-1/2, -(outputs[:,0,2] + 1/2)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,1,2]-1/2, -(outputs[:,1,2] + 1/2)), zero[:,0,0])**2 \\\n",
    "          )/6\n",
    "    return (re).mean()\n",
    "\n",
    "def realizabilityEigLoss(outputs, labels, alpha = 1):\n",
    "    eigs = torch.sort(torch.real(torch.linalg.eigvals(outputs)),descending=True)[0]\n",
    "    #print(eigs)\n",
    "    #print(eigs[:,1])\n",
    "    zero = torch.zeros_like(eigs[:,0])\n",
    "    #if torch.any((3*torch.abs(eigs[:,1])-eigs[:,1])/2 - eigs[:,0]>zero):\n",
    "    #    print('realizability error 1')\n",
    "    #if torch.any(eigs[:,0] - (1/3 - eigs[:,0])>zero):\n",
    "    #    print('realizability error 2')\n",
    "    re_eig =  (torch.maximum((3*torch.abs(eigs[:,1])-eigs[:,1])/2 - eigs[:,0],zero)**2) \\\n",
    "            + (torch.maximum(eigs[:,0] - (1/3 - eigs[:,0]),zero)**2)\n",
    "    #print(f'Condition 1: negative {(3*torch.abs(eigs[:,1])-eigs[:,1])/2 - eigs[:,0]}')\n",
    "    #print(f'Condition 2: negative {eigs[:,0] - (1/3 - eigs[:,0])}')\n",
    "    #print(f'lambda1 = {eigs[:,0]}, 1/3-lambda2 = {(1/3 - eigs[:,0])}')\n",
    "    return (re_eig).mean()\n",
    "\n",
    "def verboserealizabilityEigLoss(outputs, labels, alpha = 1):\n",
    "    eigs = torch.sort(torch.real(torch.linalg.eigvals(outputs)),descending=True)[0]\n",
    "    #print(eigs)\n",
    "    #print(eigs[:,1])\n",
    "    zero = torch.zeros_like(eigs[:,0])\n",
    "    if torch.any((3*torch.abs(eigs[:,1])-eigs[:,1])/2 - eigs[:,0]>zero):\n",
    "        print('realizability error 1')\n",
    "    if torch.any(eigs[:,0] - (1/3 - eigs[:,0])>zero):\n",
    "        print('realizability error 2')\n",
    "    re_eig =  (torch.maximum((3*torch.abs(eigs[:,1])-eigs[:,1])/2 - eigs[:,0],zero)**2) \\\n",
    "            + (torch.maximum(eigs[:,0] - (1/3 - eigs[:,0]),zero)**2)\n",
    "    print(f'Condition 1: negative {(3*torch.abs(eigs[:,1])-eigs[:,1])/2 - eigs[:,0]}')\n",
    "    print(f'Condition 2: negative {eigs[:,0] - (1/3 - eigs[:,0])}')\n",
    "    print(f'lambda1 = {eigs[:,0]}, 1/3-lambda2 = {(1/3 - eigs[:,0])}')\n",
    "    return (re_eig).mean()\n",
    "\n",
    "def realizabilityFullLoss(outputs, labels, alpha = 1):\n",
    "    eigs = torch.sort(torch.real(torch.linalg.eigvals(outputs)),descending=True)[0]\n",
    "    zero = torch.zeros_like(outputs)\n",
    "    zero_eig = torch.zeros_like(eigs[:,0])\n",
    "    re =  (torch.maximum(torch.maximum(outputs[:,0,0]-2/3, -(outputs[:,0,0] + 1/3)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,1,1]-2/3, -(outputs[:,1,1] + 1/3)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,2,2]-2/3, -(outputs[:,2,2] + 1/3)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,0,1]-1/2, -(outputs[:,0,1] + 1/2)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,0,2]-1/2, -(outputs[:,0,2] + 1/2)), zero[:,0,0])**2 \\\n",
    "        + torch.maximum(torch.maximum(outputs[:,1,2]-1/2, -(outputs[:,1,2] + 1/2)), zero[:,0,0])**2 \\\n",
    "          )/6 \\\n",
    "        + (torch.maximum((3*torch.abs(eigs[:,1])-eigs[:,1])/2 - eigs[:,0],zero_eig)**2) \\\n",
    "            + (torch.maximum(eigs[:,0] - (1/3 - eigs[:,0]),zero_eig)**2)\n",
    "    return (re).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00db0551-fa80-43c6-b4da-d30a9c59057e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch MSE:       0.0\n",
      "Custom mse:      0.0\n",
      "Equiv symm loss: 0.0\n",
      "b loss:          0.14453168213367462\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "#loss_fn2 = myCustomLoss()\n",
    "\n",
    "b_label = torch.tensor([[[-0.9266, -0.9595,  0.6728],\n",
    "  [-0.9595, -0.2660, -0.3291],\n",
    "  [ 0.6728, -0.3291,  1.1926]],\n",
    " [[ 0.2544,  0.9140,  0.2937],\n",
    "  [ 0.9140,  0.8664, -0.6828],\n",
    "  [ 0.2937, -0.6828, -1.1208]]]\n",
    ")\n",
    "\n",
    "b_pred = torch.tensor( [[[-0.3729, -0.6638, -0.7964],\n",
    "  [-0.6638, -0.9099,  0.4229],\n",
    "  [-0.7964,  0.4229,  1.2828]],\n",
    " [[ 0.3710, -0.4268,  0.3966],\n",
    "  [-0.4268, -0.3257,  0.3040],\n",
    "  [ 0.3966,  0.3040, -0.0453]]]\n",
    ")\n",
    "b_pred = b_label\n",
    "\n",
    "\n",
    "\n",
    "print(b_label-b_pred)\n",
    "\n",
    "print((b_label-b_pred)**2)\n",
    "\n",
    "print(f'torch MSE:       {loss_fn(b_label,b_pred)}')\n",
    "print(f'Custom mse:      {myCustomLoss(b_label,b_pred)}')\n",
    "print(f'Equiv symm loss: {bLoss_equiv_mse(b_label,b_pred)}')\n",
    "print(f'b loss:          {bLoss(b_label,b_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc0840f9-d2aa-49f4-83bf-9f347118e183",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3e8449f610>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b11 = np.arange(-1,1,0.001)\n",
    "plt.plot(b11, 5*np.square(np.maximum(b11-2/3,np.maximum(-(b11+1/3),0))))\n",
    "plt.plot(b11, 20*np.square(np.maximum(b11-2/3,np.maximum(-(b11+1/3),0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e2460f7-f619-4724-8838-d886e5d35fd8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.0\n",
      "\n",
      "\n",
      "Realizable: b11 = 0.19350123170647382\n",
      "Realizable: b12 = 0.09315907576505346\n",
      "Realizable: b13 = -0.1652610208161825\n",
      "Realizable: b21 = 0.09315907576505346\n",
      "Realizable: b22 = -0.056194284784299706\n",
      "Realizable: b23 = -0.10723877513320751\n",
      "Realizable: b31 = -0.1652610208161825\n",
      "Realizable: b32 = -0.10723877513320751\n",
      "Realizable: b33 = -0.13730694692217413\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.05877612237594257\n",
      "Realizable: b12 = 0.10751129558743991\n",
      "Realizable: b13 = 0.10474703304830876\n",
      "Realizable: b21 = 0.10751129558743991\n",
      "Realizable: b22 = 0.001624663347389252\n",
      "Realizable: b23 = -0.13920828011556532\n",
      "Realizable: b31 = 0.10474703304830876\n",
      "Realizable: b32 = -0.13920828011556532\n",
      "Realizable: b33 = 0.05715145902855334\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.10199634709189807\n",
      "Realizable: b12 = -0.014325732278433546\n",
      "Realizable: b13 = 0.003593085877485411\n",
      "Realizable: b21 = -0.014325732278433546\n",
      "Realizable: b22 = 0.11951284578996049\n",
      "Realizable: b23 = -0.1258699228293364\n",
      "Realizable: b31 = 0.003593085877485411\n",
      "Realizable: b32 = -0.1258699228293364\n",
      "Realizable: b33 = -0.017516498698062377\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.028980291370078046\n",
      "Realizable: b12 = -0.03765848239639603\n",
      "Realizable: b13 = -0.029335527391957616\n",
      "Realizable: b21 = -0.03765848239639603\n",
      "Realizable: b22 = 0.19322905932441756\n",
      "Realizable: b23 = 0.09424404254922047\n",
      "Realizable: b31 = -0.029335527391957616\n",
      "Realizable: b32 = 0.09424404254922047\n",
      "Realizable: b33 = -0.1642487679543395\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.09366867108867799\n",
      "Realizable: b12 = 0.14911462879128007\n",
      "Realizable: b13 = 0.13643484369357098\n",
      "Realizable: b21 = 0.14911462879128007\n",
      "Realizable: b22 = -0.05249263774909361\n",
      "Realizable: b23 = -0.1360434708558192\n",
      "Realizable: b31 = 0.13643484369357098\n",
      "Realizable: b32 = -0.1360434708558192\n",
      "Realizable: b33 = 0.14616130883777162\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.03531368319720401\n",
      "Realizable: b12 = -0.015382427807563437\n",
      "Realizable: b13 = -0.03953550410468409\n",
      "Realizable: b21 = -0.015382427807563437\n",
      "Realizable: b22 = 0.10089037714234134\n",
      "Realizable: b23 = 0.1426870403256554\n",
      "Realizable: b31 = -0.03953550410468409\n",
      "Realizable: b32 = 0.1426870403256554\n",
      "Realizable: b33 = -0.06557669394513732\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.10157666884627875\n",
      "Realizable: b12 = 0.017051735445176214\n",
      "Realizable: b13 = 0.08059167295988193\n",
      "Realizable: b21 = 0.017051735445176214\n",
      "Realizable: b22 = -0.017062530816837118\n",
      "Realizable: b23 = 0.2291680186616246\n",
      "Realizable: b31 = 0.08059167295988193\n",
      "Realizable: b32 = 0.2291680186616246\n",
      "Realizable: b33 = 0.11863919966311587\n",
      "\n",
      "\n",
      "Realizable: b11 = 0.018349143221457795\n",
      "Realizable: b12 = 0.09974382311783414\n",
      "Realizable: b13 = -0.13251287811478774\n",
      "Realizable: b21 = 0.09974382311783414\n",
      "Realizable: b22 = -0.0684298377304674\n",
      "Realizable: b23 = -0.15331105606429052\n",
      "Realizable: b31 = -0.13251287811478774\n",
      "Realizable: b32 = -0.15331105606429052\n",
      "Realizable: b33 = 0.05008069450900961\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.13057007098727177\n",
      "Realizable: b12 = 0.07506901704078256\n",
      "Realizable: b13 = -0.011968047364379942\n",
      "Realizable: b21 = 0.07506901704078256\n",
      "Realizable: b22 = -0.07432049053143626\n",
      "Realizable: b23 = 0.03827370316221999\n",
      "Realizable: b31 = -0.011968047364379942\n",
      "Realizable: b32 = 0.03827370316221999\n",
      "Realizable: b33 = 0.20489056151870805\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.17187391162667243\n",
      "Realizable: b12 = -0.022288850040813987\n",
      "Realizable: b13 = 0.04116050718127739\n",
      "Realizable: b21 = -0.022288850040813987\n",
      "Realizable: b22 = 0.0821906147988537\n",
      "Realizable: b23 = -0.10680338436360666\n",
      "Realizable: b31 = 0.04116050718127739\n",
      "Realizable: b32 = -0.10680338436360666\n",
      "Realizable: b33 = 0.08968329682781875\n"
     ]
    }
   ],
   "source": [
    "# Realizability testing: model should be able to predict only realizable points, even given large T's, with only the realizability loss criterion\n",
    "np.set_printoptions(precision=4,floatmode='fixed')\n",
    "test = 0\n",
    "tensor_N = 10\n",
    "br = np.random.uniform(-1,1,size=(2,5))\n",
    "N_points = 10\n",
    "\n",
    "def generate_random_antisymmetric_tensors(n_points):\n",
    "    b = np.empty((n_points,3,3))\n",
    "    br = np.random.uniform(-1,1,size=(n_points,5))\n",
    "    for point in range(n_points):\n",
    "        b[point,0,0] = br[point,0]\n",
    "        b[point,0,1] = br[point,1]\n",
    "        b[point,0,2] = br[point,2]\n",
    "        b[point,1,1] = br[point,3]\n",
    "        b[point,1,2] = br[point,4]\n",
    "        b[point,2,2] = -b[point,0,0] - b[point,1,1]\n",
    "        b[point,1,0] = b[point,0,1]\n",
    "        b[point,2,0] = b[point,0,2]\n",
    "        b[point,2,1] = b[point,1,2]\n",
    "    return b\n",
    "\n",
    "model = TBNN(N=tensor_N, input_dim=5, n_hidden = 5, neurons = 100)\n",
    "\n",
    "Tn = np.empty((N_points,tensor_N,3,3))\n",
    "for point in range(N_points):\n",
    "    for n in range(tensor_N):\n",
    "        Tn[point,n,:,:] = generate_random_antisymmetric_tensors(1)\n",
    "        #print(Tn[0,n,:,:] )\n",
    "x = torch.tensor(np.random.uniform(-1,1,size=(N_points,5)),dtype = torch.float32)\n",
    "Tn = torch.tensor(Tn*10)\n",
    "b_label = torch.tensor(generate_random_antisymmetric_tensors(N_points))\n",
    "#loss_fn = realizabilityLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    y_pred, gn = model(x, Tn)\n",
    "    if epoch == 0:\n",
    "        gn0 = gn\n",
    "    loss = realizabilityLoss(y_pred.float(), b_label.float())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "predictions,gn = model(x,Tn)\n",
    "loss = realizabilityLoss(predictions.float(), b_label.float())\n",
    "print(f'LOSS: {loss}')\n",
    "for point in range(N_points): \n",
    "    print('\\n')\n",
    "    for i in range(0,3):\n",
    "        for j in range(0,3):\n",
    "            if i == j:\n",
    "                if predictions[point][i,j] > 2/3 or predictions[point][i,j]<-1/3:\n",
    "                    print(f'REALIZABILITY FAILED: b{i+1}{j+1} = {predictions[point][i,j]}')\n",
    "                else:\n",
    "                    print(f'Realizable: b{i+1}{j+1} = {predictions[point][i,j]}')\n",
    "            else: \n",
    "                if predictions[point][i,j] > 1/2 or predictions[point][i,j]<-1/2:\n",
    "                    print(f'REALIZABILITY FAILED: b{i+1}{j+1} = {predictions[point][i,j]}')\n",
    "                else:\n",
    "                    print(f'Realizable: b{i+1}{j+1} = {predictions[point][i,j]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fc249244-5255-4191-a826-1d92654d4ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realizability error 2\n",
      "Condition 1: negative tensor([-0.6010, -0.6870, -0.4884], dtype=torch.float64)\n",
      "Condition 2: negative tensor([3.0849, 1.8264, 2.0122], dtype=torch.float64)\n",
      "lambda1 = tensor([1.7091, 1.0799, 1.1727], dtype=torch.float64), 1/3-lambda2 = tensor([-1.3758, -0.7465, -0.8394], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "b_label = torch.tensor([[[-0.9266, -0.9595,  0.6728],\n",
    "  [-0.9595, -0.2660, -0.3291],\n",
    "  [ 0.6728, -0.3291,  1.1926]],\n",
    " [[ 0.2544,  0.9140,  0.2937],\n",
    "  [ 0.9140,  0.8664, -0.6828],\n",
    "  [ 0.2937, -0.6828, -1.1208]]]\n",
    ")\n",
    "\n",
    "b_pred = torch.tensor( [[[-0.3729, -0.6638, -0.7964],\n",
    "  [-0.6638, -0.9099,  0.4229],\n",
    "  [-0.7964,  0.4229,  1.2828]],\n",
    " [[ 0.3710, -0.4268,  0.3966],\n",
    "  [-0.4268, -0.3257,  0.3040],\n",
    "  [ 0.3966,  0.3040, -0.0453]]]\n",
    ")\n",
    "\n",
    "loss = 0.0\n",
    "while loss == 0:\n",
    "    b_label = torch.tensor(generate_random_antisymmetric_tensors(3))\n",
    "    b_pred = torch.tensor(generate_random_antisymmetric_tensors(3))\n",
    "    loss = realizabilityEigLoss(b_pred,b_label)\n",
    "\n",
    "#print(realizabilityEigLoss(b_pred,b_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "279983cf-c0f5-4773-ad2a-b66744fbaad5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realizability error 2\n",
      "Condition 1: negative tensor([-2.3589, -1.8967, -1.5408, -1.6887, -0.7248, -2.3956, -1.1776, -1.0277,\n",
      "        -1.8560, -3.0236], grad_fn=<SubBackward0>)\n",
      "Condition 2: negative tensor([4.8764, 5.2417, 4.0394, 6.6526, 3.6441, 4.6776, 6.3912, 4.4254, 6.2655,\n",
      "        6.0138], grad_fn=<SubBackward0>)\n",
      "lambda1 = tensor([2.6049, 2.7875, 2.1864, 3.4930, 1.9887, 2.5055, 3.3623, 2.3793, 3.2994,\n",
      "        3.1735], grad_fn=<SelectBackward0>), 1/3-lambda2 = tensor([-2.2715, -2.4542, -1.8530, -3.1596, -1.6554, -2.1721, -3.0289, -2.0460,\n",
      "        -2.9661, -2.8402], grad_fn=<RsubBackward1>)\n",
      "LOSS: 28.284103393554688\n",
      "After training:\n",
      "Condition 1: negative tensor([-0.1373, -0.0949, -0.1091, -0.1123, -0.1164, -0.1244, -0.0681, -0.0196,\n",
      "        -0.0485, -0.1108], grad_fn=<SubBackward0>)\n",
      "Condition 2: negative tensor([-0.0480, -0.0837, -0.0763, -0.0981, -0.0955, -0.0513, -0.0174, -0.2046,\n",
      "        -0.1821, -0.0877], grad_fn=<SubBackward0>)\n",
      "lambda1 = tensor([0.1427, 0.1248, 0.1285, 0.1176, 0.1189, 0.1410, 0.1580, 0.0644, 0.0756,\n",
      "        0.1228], grad_fn=<SelectBackward0>), 1/3-lambda2 = tensor([0.1907, 0.2085, 0.2048, 0.2157, 0.2144, 0.1923, 0.1753, 0.2690, 0.2577,\n",
      "        0.2105], grad_fn=<RsubBackward1>)\n",
      "LOSS: 0.0\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.07476411155247867\n",
      "Realizable: b12 = -0.06698366323368264\n",
      "Realizable: b13 = -0.022460318944612208\n",
      "Realizable: b21 = -0.06698366323368264\n",
      "Realizable: b22 = -0.0648937632892315\n",
      "Realizable: b23 = -0.018198975585657824\n",
      "Realizable: b31 = -0.022460318944612208\n",
      "Realizable: b32 = -0.018198975585657824\n",
      "Realizable: b33 = 0.1396578748417102\n",
      "\n",
      "\n",
      "Realizable: b11 = 0.052953904868401\n",
      "Realizable: b12 = -0.05467302524619761\n",
      "Realizable: b13 = -0.09322801910352473\n",
      "Realizable: b21 = -0.05467302524619761\n",
      "Realizable: b22 = -0.024607578450623294\n",
      "Realizable: b23 = -0.012053511987499179\n",
      "Realizable: b31 = -0.09322801910352473\n",
      "Realizable: b32 = -0.012053511987499179\n",
      "Realizable: b33 = -0.028346326417777675\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.029671168049118977\n",
      "Realizable: b12 = -0.04503017315687957\n",
      "Realizable: b13 = 0.08739602380114012\n",
      "Realizable: b21 = -0.04503017315687957\n",
      "Realizable: b22 = -0.050520127698166085\n",
      "Realizable: b23 = 0.02773180253762174\n",
      "Realizable: b31 = 0.08739602380114012\n",
      "Realizable: b32 = 0.02773180253762174\n",
      "Realizable: b33 = 0.08019129574728508\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.05879135691775552\n",
      "Realizable: b12 = -0.09371372457456512\n",
      "Realizable: b13 = 0.02684396713161805\n",
      "Realizable: b21 = -0.09371372457456512\n",
      "Realizable: b22 = 0.06453221212125525\n",
      "Realizable: b23 = 0.03409922991001224\n",
      "Realizable: b31 = 0.02684396713161805\n",
      "Realizable: b32 = 0.03409922991001224\n",
      "Realizable: b33 = -0.005740855203499725\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.013454444060618541\n",
      "Realizable: b12 = 0.04769938617733496\n",
      "Realizable: b13 = 0.04580829332948952\n",
      "Realizable: b21 = 0.04769938617733496\n",
      "Realizable: b22 = -0.08887465441198789\n",
      "Realizable: b23 = -0.02818290253628593\n",
      "Realizable: b31 = 0.04580829332948952\n",
      "Realizable: b32 = -0.02818290253628593\n",
      "Realizable: b33 = 0.10232909847260643\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.08578297740392461\n",
      "Realizable: b12 = -0.08261139908610599\n",
      "Realizable: b13 = -0.028642892536461354\n",
      "Realizable: b21 = -0.08261139908610599\n",
      "Realizable: b22 = -0.04987181821592484\n",
      "Realizable: b23 = -0.02724110163033581\n",
      "Realizable: b31 = -0.028642892536461354\n",
      "Realizable: b32 = -0.02724110163033581\n",
      "Realizable: b33 = 0.13565479561984947\n",
      "\n",
      "\n",
      "Realizable: b11 = 0.1546127108680437\n",
      "Realizable: b12 = -0.015512964211866348\n",
      "Realizable: b13 = -0.009339081608313426\n",
      "Realizable: b21 = -0.015512964211866348\n",
      "Realizable: b22 = -0.2073071647612845\n",
      "Realizable: b23 = 0.10988656535391914\n",
      "Realizable: b31 = -0.009339081608313426\n",
      "Realizable: b32 = 0.10988656535391914\n",
      "Realizable: b33 = 0.05269445389324079\n",
      "\n",
      "\n",
      "Realizable: b11 = 0.06090801046715245\n",
      "Realizable: b12 = 0.007732490588054704\n",
      "Realizable: b13 = -0.02064114652712519\n",
      "Realizable: b21 = 0.007732490588054704\n",
      "Realizable: b22 = 0.03981704076252057\n",
      "Realizable: b23 = 0.028769729464039245\n",
      "Realizable: b31 = -0.02064114652712519\n",
      "Realizable: b32 = 0.028769729464039245\n",
      "Realizable: b33 = -0.100725051229673\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.031358386974042834\n",
      "Realizable: b12 = -0.016452484450759485\n",
      "Realizable: b13 = 0.03133655890907666\n",
      "Realizable: b21 = -0.016452484450759485\n",
      "Realizable: b22 = -0.028805017003124288\n",
      "Realizable: b23 = 0.03011936850608453\n",
      "Realizable: b31 = 0.03133655890907666\n",
      "Realizable: b32 = 0.03011936850608453\n",
      "Realizable: b33 = 0.060163403977167115\n",
      "\n",
      "\n",
      "Realizable: b11 = 0.024314528731617845\n",
      "Realizable: b12 = -0.06069691342442012\n",
      "Realizable: b13 = 0.00035376567321779295\n",
      "Realizable: b21 = -0.06069691342442012\n",
      "Realizable: b22 = 0.0648964206846209\n",
      "Realizable: b23 = -0.06568347660373965\n",
      "Realizable: b31 = 0.00035376567321779295\n",
      "Realizable: b32 = -0.06568347660373965\n",
      "Realizable: b33 = -0.08921094941623872\n"
     ]
    }
   ],
   "source": [
    "# Realizability testing: model should be able to predict only realizable points, even given large T's, with only the realizability loss criterion\n",
    "np.set_printoptions(precision=4,floatmode='fixed')\n",
    "test = 0\n",
    "tensor_N = 10\n",
    "br = np.random.uniform(-1,1,size=(2,5))\n",
    "N_points = 10\n",
    "\n",
    "def generate_random_antisymmetric_tensors(n_points):\n",
    "    b = np.empty((n_points,3,3))\n",
    "    br = np.random.uniform(-1,1,size=(n_points,5))\n",
    "    for point in range(n_points):\n",
    "        b[point,0,0] = br[point,0]\n",
    "        b[point,0,1] = br[point,1]\n",
    "        b[point,0,2] = br[point,2]\n",
    "        b[point,1,1] = br[point,3]\n",
    "        b[point,1,2] = br[point,4]\n",
    "        b[point,2,2] = -b[point,0,0] - b[point,1,1]\n",
    "        b[point,1,0] = b[point,0,1]\n",
    "        b[point,2,0] = b[point,0,2]\n",
    "        b[point,2,1] = b[point,1,2]\n",
    "    return b\n",
    "\n",
    "model = TBNN(N=tensor_N, input_dim=5, n_hidden = 5, neurons = 100)\n",
    "\n",
    "Tn = np.empty((N_points,tensor_N,3,3))\n",
    "for point in range(N_points):\n",
    "    for n in range(tensor_N):\n",
    "        Tn[point,n,:,:] = generate_random_antisymmetric_tensors(1)\n",
    "        #print(Tn[0,n,:,:] )\n",
    "x = torch.tensor(np.random.uniform(-1,1,size=(N_points,5)),dtype = torch.float32)\n",
    "Tn = torch.tensor(Tn*10)\n",
    "b_label = torch.tensor(generate_random_antisymmetric_tensors(N_points))\n",
    "#loss_fn = realizabilityLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "y_pred, gn = model(x, Tn)\n",
    "loss = verboserealizabilityEigLoss(y_pred.float(), b_label.float())\n",
    "print(f'LOSS: {loss}')\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    y_pred, gn = model(x, Tn)\n",
    "    if epoch == 0:\n",
    "        gn0 = gn\n",
    "    loss = realizabilityEigLoss(y_pred.float(), b_label.float())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "predictions,gn = model(x,Tn)\n",
    "print('After training:')\n",
    "loss = verboserealizabilityEigLoss(predictions.float(), b_label.float())\n",
    "print(f'LOSS: {loss}')\n",
    "for point in range(N_points): \n",
    "    print('\\n')\n",
    "    for i in range(0,3):\n",
    "        for j in range(0,3):\n",
    "            if i == j:\n",
    "                if predictions[point][i,j] > 2/3 or predictions[point][i,j]<-1/3:\n",
    "                    print(f'REALIZABILITY FAILED: b{i+1}{j+1} = {predictions[point][i,j]}')\n",
    "                else:\n",
    "                    print(f'Realizable: b{i+1}{j+1} = {predictions[point][i,j]}')\n",
    "            else: \n",
    "                if predictions[point][i,j] > 1/2 or predictions[point][i,j]<-1/2:\n",
    "                    print(f'REALIZABILITY FAILED: b{i+1}{j+1} = {predictions[point][i,j]}')\n",
    "                else:\n",
    "                    print(f'Realizable: b{i+1}{j+1} = {predictions[point][i,j]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b933e341-8e1c-4b60-8031-396e88d89ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 9.804380416870117\n",
      "After training:\n",
      "LOSS: 0.0\n",
      "Condition 1: negative tensor([-0.1121, -0.0891, -0.1022, -0.0913, -0.0730, -0.0942, -0.0416, -0.0805,\n",
      "        -0.0782, -0.0533], grad_fn=<SubBackward0>)\n",
      "Condition 2: negative tensor([-0.0846, -0.1010, -0.1193, -0.0333, -0.1243, -0.0846, -0.0595, -0.1278,\n",
      "        -0.1631, -0.2177], grad_fn=<SubBackward0>)\n",
      "lambda1 = tensor([0.1244, 0.1162, 0.1070, 0.1500, 0.1045, 0.1244, 0.1369, 0.1028, 0.0851,\n",
      "        0.0578], grad_fn=<SelectBackward0>), 1/3-lambda2 = tensor([0.2089, 0.2171, 0.2263, 0.1833, 0.2288, 0.2089, 0.1964, 0.2305, 0.2482,\n",
      "        0.2755], grad_fn=<RsubBackward1>)\n",
      "verbose LOSS: 0.0\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.10453618503725141\n",
      "Realizable: b12 = 0.04618557290716444\n",
      "Realizable: b13 = -0.042389234325317084\n",
      "Realizable: b21 = 0.04618557290716444\n",
      "Realizable: b22 = 0.08130364814994266\n",
      "Realizable: b23 = 0.06467997594624128\n",
      "Realizable: b31 = -0.042389234325317084\n",
      "Realizable: b32 = 0.06467997594624128\n",
      "Realizable: b33 = 0.02323253688730876\n",
      "\n",
      "\n",
      "Realizable: b11 = 0.02717528634521494\n",
      "Realizable: b12 = -0.04851386594873388\n",
      "Realizable: b13 = -0.03693526780583747\n",
      "Realizable: b21 = -0.04851386594873388\n",
      "Realizable: b22 = 0.07899536517020335\n",
      "Realizable: b23 = -0.06731762761258397\n",
      "Realizable: b31 = -0.03693526780583747\n",
      "Realizable: b32 = -0.06731762761258397\n",
      "Realizable: b33 = -0.1061706515154183\n",
      "\n",
      "\n",
      "Realizable: b11 = 0.032083132028850686\n",
      "Realizable: b12 = 0.03208878863696829\n",
      "Realizable: b13 = -0.06238496941822446\n",
      "Realizable: b21 = 0.03208878863696829\n",
      "Realizable: b22 = -0.08663660837164225\n",
      "Realizable: b23 = 0.03612228918674616\n",
      "Realizable: b31 = -0.06238496941822446\n",
      "Realizable: b32 = 0.03612228918674616\n",
      "Realizable: b33 = 0.054553476342791576\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.027172081620366807\n",
      "Realizable: b12 = 0.01694168945415586\n",
      "Realizable: b13 = 0.12000426207951048\n",
      "Realizable: b21 = 0.01694168945415586\n",
      "Realizable: b22 = -0.016032028966524135\n",
      "Realizable: b23 = 0.05332416486890461\n",
      "Realizable: b31 = 0.12000426207951048\n",
      "Realizable: b32 = 0.05332416486890461\n",
      "Realizable: b33 = 0.04320411058689094\n",
      "\n",
      "\n",
      "Realizable: b11 = 0.016205443665299628\n",
      "Realizable: b12 = 0.08887119625125622\n",
      "Realizable: b13 = -0.016409910725982583\n",
      "Realizable: b21 = 0.08887119625125622\n",
      "Realizable: b22 = -0.07790640228842843\n",
      "Realizable: b23 = -0.04425587328364901\n",
      "Realizable: b31 = -0.016409910725982583\n",
      "Realizable: b32 = -0.04425587328364901\n",
      "Realizable: b33 = 0.0617009586231288\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.1046131611347136\n",
      "Realizable: b12 = 0.05645690170767337\n",
      "Realizable: b13 = 0.06684991662467452\n",
      "Realizable: b21 = 0.05645690170767337\n",
      "Realizable: b22 = 0.012488376020337026\n",
      "Realizable: b23 = -0.0518189356814822\n",
      "Realizable: b31 = 0.06684991662467452\n",
      "Realizable: b32 = -0.0518189356814822\n",
      "Realizable: b33 = 0.09212478511437658\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.02955701792418614\n",
      "Realizable: b12 = -0.053153455236039815\n",
      "Realizable: b13 = -0.026141282473910073\n",
      "Realizable: b21 = -0.053153455236039815\n",
      "Realizable: b22 = 0.04603318514239031\n",
      "Realizable: b23 = 0.09671341202863662\n",
      "Realizable: b31 = -0.026141282473910073\n",
      "Realizable: b32 = 0.09671341202863662\n",
      "Realizable: b33 = -0.01647616721820416\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.025979764887283315\n",
      "Realizable: b12 = -0.03308605153261786\n",
      "Realizable: b13 = -0.0730587555510009\n",
      "Realizable: b21 = -0.03308605153261786\n",
      "Realizable: b22 = 0.008002410878449629\n",
      "Realizable: b23 = -0.07994136131206253\n",
      "Realizable: b31 = -0.0730587555510009\n",
      "Realizable: b32 = -0.07994136131206253\n",
      "Realizable: b33 = 0.01797735400883369\n",
      "\n",
      "\n",
      "Realizable: b11 = -0.005686013421473124\n",
      "Realizable: b12 = -0.04940038921811247\n",
      "Realizable: b13 = -0.03336293010525165\n",
      "Realizable: b21 = -0.04940038921811247\n",
      "Realizable: b22 = 0.05378338772242801\n",
      "Realizable: b23 = -0.041373075893820846\n",
      "Realizable: b31 = -0.03336293010525165\n",
      "Realizable: b32 = -0.041373075893820846\n",
      "Realizable: b33 = -0.048097374300954926\n",
      "\n",
      "\n",
      "Realizable: b11 = 0.05745841708152928\n",
      "Realizable: b12 = 0.004734617316554564\n",
      "Realizable: b13 = 0.0033895646044819275\n",
      "Realizable: b21 = 0.004734617316554564\n",
      "Realizable: b22 = -0.008592032962110398\n",
      "Realizable: b23 = -0.017419779552593162\n",
      "Realizable: b31 = 0.0033895646044819275\n",
      "Realizable: b32 = -0.017419779552593162\n",
      "Realizable: b33 = -0.04886638411941888\n"
     ]
    }
   ],
   "source": [
    "# Realizability testing: model should be able to predict only realizable points, even given large T's, with only the realizability loss criterion\n",
    "np.set_printoptions(precision=4,floatmode='fixed')\n",
    "test = 0\n",
    "tensor_N = 10\n",
    "br = np.random.uniform(-1,1,size=(2,5))\n",
    "N_points = 10\n",
    "\n",
    "def generate_random_antisymmetric_tensors(n_points):\n",
    "    b = np.empty((n_points,3,3))\n",
    "    br = np.random.uniform(-1,1,size=(n_points,5))\n",
    "    for point in range(n_points):\n",
    "        b[point,0,0] = br[point,0]\n",
    "        b[point,0,1] = br[point,1]\n",
    "        b[point,0,2] = br[point,2]\n",
    "        b[point,1,1] = br[point,3]\n",
    "        b[point,1,2] = br[point,4]\n",
    "        b[point,2,2] = -b[point,0,0] - b[point,1,1]\n",
    "        b[point,1,0] = b[point,0,1]\n",
    "        b[point,2,0] = b[point,0,2]\n",
    "        b[point,2,1] = b[point,1,2]\n",
    "    return b\n",
    "\n",
    "model = TBNN(N=tensor_N, input_dim=5, n_hidden = 5, neurons = 100)\n",
    "\n",
    "Tn = np.empty((N_points,tensor_N,3,3))\n",
    "for point in range(N_points):\n",
    "    for n in range(tensor_N):\n",
    "        Tn[point,n,:,:] = generate_random_antisymmetric_tensors(1)\n",
    "        #print(Tn[0,n,:,:] )\n",
    "x = torch.tensor(np.random.uniform(-1,1,size=(N_points,5)),dtype = torch.float32)\n",
    "Tn = torch.tensor(Tn*10)\n",
    "b_label = torch.tensor(generate_random_antisymmetric_tensors(N_points))\n",
    "#loss_fn = realizabilityLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "y_pred, gn = model(x, Tn)\n",
    "loss = realizabilityFullLoss(y_pred.float(), b_label.float())\n",
    "print(f'LOSS: {loss}')\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    y_pred, gn = model(x, Tn)\n",
    "    if epoch == 0:\n",
    "        gn0 = gn\n",
    "    loss = realizabilityFullLoss(y_pred.float(), b_label.float())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "predictions,gn = model(x,Tn)\n",
    "print('After training:')\n",
    "loss = realizabilityFullLoss(predictions.float(), b_label.float())\n",
    "print(f'LOSS: {loss}')\n",
    "loss = verboserealizabilityEigLoss(predictions.float(), b_label.float())\n",
    "print(f'verbose LOSS: {loss}')\n",
    "for point in range(N_points): \n",
    "    print('\\n')\n",
    "    for i in range(0,3):\n",
    "        for j in range(0,3):\n",
    "            if i == j:\n",
    "                if predictions[point][i,j] > 2/3 or predictions[point][i,j]<-1/3:\n",
    "                    print(f'REALIZABILITY FAILED: b{i+1}{j+1} = {predictions[point][i,j]}')\n",
    "                else:\n",
    "                    print(f'Realizable: b{i+1}{j+1} = {predictions[point][i,j]}')\n",
    "            else: \n",
    "                if predictions[point][i,j] > 1/2 or predictions[point][i,j]<-1/2:\n",
    "                    print(f'REALIZABILITY FAILED: b{i+1}{j+1} = {predictions[point][i,j]}')\n",
    "                else:\n",
    "                    print(f'Realizable: b{i+1}{j+1} = {predictions[point][i,j]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f827d0f-8c49-4351-8c95-92b07064e79d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
